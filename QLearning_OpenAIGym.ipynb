{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "* gym - collection of environments for reinforcement learning algorithms.\n",
    "* numpy - package for scientific computing with Python. \n",
    "* random - implements pseudo-random number generators for various distributions.\n",
    "* math -  mathematical functions defined by the C standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the environment\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pole starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the environment related constants.\n",
    "*  Every environment comes with an action_space and an observation_space. These attributes are of type Space, and they describe the format of valid actions and observations. The Discrete space allows a fixed range of non-negative numbers, so in this case valid actions are either 0 or 1. The Box space represents an n-dimensional box, so valid observations will be an array of 4 numbers.\n",
    "* Define the following constants:\n",
    "   * Number of discrete states (bucket) per state dimension :position,velocity,angle between the pole and the vertical axis and the angular velocity. \n",
    "   * Number of discrete actions (left, right) \n",
    "   * Bounds for each discrete state\n",
    "* Define the minimum exploration and learning rate values.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.low "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BUCKETS = (1, 1, 6, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTIONS = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_BOUNDS = list(zip(env.observation_space.low, env.observation_space.high))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_BOUNDS[1] = [-0.5, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_BOUNDS[3] = [-math.radians(50), math.radians(50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_BOUNDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Q-Table\n",
    "* Create a Q-Table for each state-action pair\n",
    "* Define the learning related constants:\n",
    "* The learning rate must decay but not too fast. The conditions for convergence are the following:\n",
    "    *sum(alpha(t), 1, inf) = inf\n",
    "    *sum(alpha(t)^2, 1, inf) < inf\n",
    "    * Something like alpha = k/(k+t) can work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros(NUM_BUCKETS + (NUM_ACTIONS,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLORE_RATE_MIN = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE_MIN = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_explore_rate(t):\n",
    "    return max(EXPLORE_RATE_MIN, min(1, 1.0 - math.log10((t+1)/25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learning_rate(t):\n",
    "    return max(LEARNING_RATE_MIN, min(0.5, 1.0 - math.log10((t+1)/25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define method for selecting an action\n",
    "* Select a random action or the action with the highest q based on explore rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, explore_rate):\n",
    "    if random.random() < explore_rate:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(q_table[state])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define method to define states\n",
    "* Discretize the continuous dimensions to a number of buckets. \n",
    "* Map the state bounds to the bucket array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_bucket(state):\n",
    "    \n",
    "    bucket_indices = []\n",
    "    \n",
    "    for i in range(len(state)):\n",
    "        if state[i] <= STATE_BOUNDS[i][0]:\n",
    "            bucket_index = 0\n",
    "            \n",
    "        elif state[i] >= STATE_BOUNDS[i][1]:\n",
    "            bucket_index = NUM_BUCKETS[i] - 1\n",
    "            \n",
    "        else:\n",
    "            bound_width = STATE_BOUNDS[i][1] - STATE_BOUNDS[i][0]\n",
    "            \n",
    "            offset = (NUM_BUCKETS[i] - 1) * STATE_BOUNDS[i][0] / bound_width\n",
    "            scaling = (NUM_BUCKETS[i] - 1) / bound_width\n",
    "            \n",
    "            bucket_index = int(round(scaling * state[i] - offset))\n",
    "            \n",
    "        bucket_indices.append(bucket_index)\n",
    "    return tuple(bucket_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define method for simulation\n",
    "* Get the initial learning and explore rates\n",
    "* Intialise discount factor and number of streaks\n",
    "* Train over 1000 episodes. In each episode, do the following:\n",
    "* Start the process using the reset method\n",
    "* Get the initial state by passing in the current observations about the environment\n",
    "* Iterate over 250 timesteps:\n",
    "    * Invoke the render method to visualise the episode.\n",
    "    * Execute a selected action using the step emthod and read the values of reward, observation, done and info.\n",
    "    * Identify the resultant state from the observations.\n",
    "    * Identify the state with the highest possible reward from the q table.\n",
    "    * Compute the q value and update the q table : Q[s, a] = Q[s, a] + alpha*(R + gamma*Max[Q(sâ€™, A)] - Q[s, a])\n",
    "    \n",
    "    * Set the new state as current state.\n",
    "    * If done equals true, episode has terminated.Reset the environment again.\n",
    "    * The agent wouldn't need more than 200 timesteps to train. Increment the no_streaks value so we can terminate training in 120 episodes, each of no more than 200 timesteps.\n",
    "    * Update the parameters so the learning and exploration rates decay as the episodes increase\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate():\n",
    "\n",
    "    learning_rate = get_learning_rate(0)\n",
    "    explore_rate = get_explore_rate(0)\n",
    "\n",
    "    discount_factor = 0.99  \n",
    "    num_streaks = 0\n",
    "\n",
    "    for episode in range(1000):\n",
    "        \n",
    "        observ = env.reset()\n",
    "    \n",
    "        state_0 = state_to_bucket(observ)\n",
    "\n",
    "        for t in range(250):\n",
    "\n",
    "            env.render()\n",
    "            \n",
    "            action = select_action(state_0, explore_rate)\n",
    "            \n",
    "            observ, reward, done, _ = env.step(action)\n",
    "\n",
    "            state = state_to_bucket(observ)\n",
    "            \n",
    "            best_q = np.amax(q_table[state])\n",
    "            \n",
    "            q_table[state_0 + (action,)] +=\\\n",
    "                learning_rate * (reward + discount_factor*(best_q) - q_table[state_0 + (action,)])\n",
    "\n",
    "            state_0 = state\n",
    "\n",
    "            print(\"\\nEpisode = %d\" % episode)\n",
    "            print(\"t = %d\" % t)\n",
    "            print(\"Action: %d\" % action)\n",
    "            print(\"State: %s\" % str(state))\n",
    "            print(\"Reward: %f\" % reward)\n",
    "            print(\"Best Q: %f\" % best_q)\n",
    "            print(\"Explore rate: %f\" % explore_rate)\n",
    "            print(\"Learning rate: %f\" % learning_rate)\n",
    "            print(\"Streaks: %d\" % num_streaks)\n",
    "\n",
    "            print(\"\")\n",
    "\n",
    "            if done:\n",
    "                print(\"Episode %d finished after %f time steps\" % (episode, t))\n",
    "                if (t >= 199):\n",
    "                    num_streaks += 1\n",
    "                else:\n",
    "                    num_streaks = 0\n",
    "                break\n",
    "\n",
    "        if num_streaks > 120:\n",
    "            break\n",
    "      \n",
    "        explore_rate = get_explore_rate(episode)\n",
    "        learning_rate = get_learning_rate(episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agent\n",
    "* Invoke the simulate method\n",
    "* Close the rendered environment.\n",
    "* Check the values in q talbe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
